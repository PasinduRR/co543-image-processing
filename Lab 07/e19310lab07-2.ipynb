{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lab 07 - Convolutional Neural Networks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Step 1: Install the necessary libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install torch torchvision"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Step 2: Import required libraries and their functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn \n",
    "import torch.optim as optim \n",
    "import torchvision\n",
    "import torchvision.transforms as transforms \n",
    "from torchvision import models\n",
    "from torch.utils.data import DataLoader\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Step 3: Load and Preprocess the MNIST Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading http://yann.lecun.com/exdb/mnist/train-images-idx3-ubyte.gz\n",
      "Failed to download (trying next):\n",
      "HTTP Error 403: Forbidden\n",
      "\n",
      "Downloading https://ossci-datasets.s3.amazonaws.com/mnist/train-images-idx3-ubyte.gz\n",
      "Downloading https://ossci-datasets.s3.amazonaws.com/mnist/train-images-idx3-ubyte.gz to ./data\\MNIST\\raw\\train-images-idx3-ubyte.gz\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100.0%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting ./data\\MNIST\\raw\\train-images-idx3-ubyte.gz to ./data\\MNIST\\raw\n",
      "\n",
      "Downloading http://yann.lecun.com/exdb/mnist/train-labels-idx1-ubyte.gz\n",
      "Failed to download (trying next):\n",
      "HTTP Error 403: Forbidden\n",
      "\n",
      "Downloading https://ossci-datasets.s3.amazonaws.com/mnist/train-labels-idx1-ubyte.gz\n",
      "Downloading https://ossci-datasets.s3.amazonaws.com/mnist/train-labels-idx1-ubyte.gz to ./data\\MNIST\\raw\\train-labels-idx1-ubyte.gz\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100.0%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting ./data\\MNIST\\raw\\train-labels-idx1-ubyte.gz to ./data\\MNIST\\raw\n",
      "\n",
      "Downloading http://yann.lecun.com/exdb/mnist/t10k-images-idx3-ubyte.gz\n",
      "Failed to download (trying next):\n",
      "HTTP Error 403: Forbidden\n",
      "\n",
      "Downloading https://ossci-datasets.s3.amazonaws.com/mnist/t10k-images-idx3-ubyte.gz\n",
      "Downloading https://ossci-datasets.s3.amazonaws.com/mnist/t10k-images-idx3-ubyte.gz to ./data\\MNIST\\raw\\t10k-images-idx3-ubyte.gz\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100.0%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting ./data\\MNIST\\raw\\t10k-images-idx3-ubyte.gz to ./data\\MNIST\\raw\n",
      "\n",
      "Downloading http://yann.lecun.com/exdb/mnist/t10k-labels-idx1-ubyte.gz\n",
      "Failed to download (trying next):\n",
      "HTTP Error 403: Forbidden\n",
      "\n",
      "Downloading https://ossci-datasets.s3.amazonaws.com/mnist/t10k-labels-idx1-ubyte.gz\n",
      "Downloading https://ossci-datasets.s3.amazonaws.com/mnist/t10k-labels-idx1-ubyte.gz to ./data\\MNIST\\raw\\t10k-labels-idx1-ubyte.gz\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100.0%"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting ./data\\MNIST\\raw\\t10k-labels-idx1-ubyte.gz to ./data\\MNIST\\raw\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "transform = transforms.Compose([\n",
    "  transforms.Resize((224, 224)),\n",
    "  transforms.ToTensor(),\n",
    "  transforms.Normalize(mean=[0.5], std=[0.5])\n",
    "])\n",
    "\n",
    "train_dataset = torchvision.datasets.MNIST(root='./data', train=True, download=True, transform=transform)\n",
    "test_dataset = torchvision.datasets.MNIST(root='./data', train=False, download=True, transform=transform)\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True)\n",
    "test_loader = DataLoader(test_dataset, batch_size=32, shuffle=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Step 4: Load the Pre-trained VGG Model and Modify It"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MNISTResNet(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(MNISTResNet, self).__init__()\n",
    "        self.resnet = models.resnet18(weights=models.ResNet18_Weights.IMAGENET1K_V1)\n",
    "        self.resnet.conv1 = nn.Conv2d(1, 64, kernel_size=7, stride=2, padding=3, bias=False)\n",
    "        num_ftrs = self.resnet.fc.in_features\n",
    "        self.resnet.fc = nn.Linear(num_ftrs, 10)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.resnet(x)\n",
    "        return x\n",
    "\n",
    "class MNISTAlexNet(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(MNISTAlexNet, self).__init__()\n",
    "        self.alexnet = models.alexnet(weights=models.AlexNet_Weights.IMAGENET1K_V1)\n",
    "        self.alexnet.features[0] = nn.Conv2d(1, 64, kernel_size=11, stride=4, padding=2)\n",
    "        num_ftrs = self.alexnet.classifier[6].in_features\n",
    "        self.alexnet.classifier[6] = nn.Linear(num_ftrs, 10)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.alexnet(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Step 5: Define the Loss Function and optimizer and initialize the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "\n",
    "resnet_model = MNISTResNet().to(device)\n",
    "alexnet_model = MNISTAlexNet().to(device)\n",
    "\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "resnet_optimizer = optim.Adam(resnet_model.parameters(), lr=0.001)\n",
    "alexnet_optimizer = optim.Adam(alexnet_model.parameters(), lr=0.001)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Step 6: Train the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "internel Epoch [1/10], Loss: 0.0013\n",
      "internel Epoch [1/10], Loss: 0.0021\n",
      "internel Epoch [1/10], Loss: 0.0027\n",
      "internel Epoch [1/10], Loss: 0.0032\n",
      "internel Epoch [1/10], Loss: 0.0035\n",
      "internel Epoch [1/10], Loss: 0.0039\n",
      "internel Epoch [1/10], Loss: 0.0040\n",
      "internel Epoch [1/10], Loss: 0.0041\n",
      "internel Epoch [1/10], Loss: 0.0043\n",
      "internel Epoch [1/10], Loss: 0.0044\n",
      "internel Epoch [1/10], Loss: 0.0047\n",
      "internel Epoch [1/10], Loss: 0.0048\n",
      "internel Epoch [1/10], Loss: 0.0050\n",
      "internel Epoch [1/10], Loss: 0.0052\n",
      "internel Epoch [1/10], Loss: 0.0053\n",
      "internel Epoch [1/10], Loss: 0.0054\n",
      "internel Epoch [1/10], Loss: 0.0055\n",
      "internel Epoch [1/10], Loss: 0.0056\n",
      "internel Epoch [1/10], Loss: 0.0057\n",
      "internel Epoch [1/10], Loss: 0.0058\n",
      "internel Epoch [1/10], Loss: 0.0060\n",
      "internel Epoch [1/10], Loss: 0.0061\n",
      "internel Epoch [1/10], Loss: 0.0065\n",
      "internel Epoch [1/10], Loss: 0.0067\n",
      "internel Epoch [1/10], Loss: 0.0069\n",
      "internel Epoch [1/10], Loss: 0.0069\n",
      "internel Epoch [1/10], Loss: 0.0071\n",
      "internel Epoch [1/10], Loss: 0.0073\n",
      "internel Epoch [1/10], Loss: 0.0074\n",
      "internel Epoch [1/10], Loss: 0.0075\n",
      "internel Epoch [1/10], Loss: 0.0075\n",
      "internel Epoch [1/10], Loss: 0.0077\n",
      "internel Epoch [1/10], Loss: 0.0077\n",
      "internel Epoch [1/10], Loss: 0.0078\n",
      "internel Epoch [1/10], Loss: 0.0079\n",
      "internel Epoch [1/10], Loss: 0.0081\n",
      "internel Epoch [1/10], Loss: 0.0084\n",
      "internel Epoch [1/10], Loss: 0.0084\n",
      "internel Epoch [1/10], Loss: 0.0085\n",
      "internel Epoch [1/10], Loss: 0.0086\n",
      "internel Epoch [1/10], Loss: 0.0088\n",
      "internel Epoch [1/10], Loss: 0.0089\n",
      "internel Epoch [1/10], Loss: 0.0090\n",
      "internel Epoch [1/10], Loss: 0.0091\n",
      "internel Epoch [1/10], Loss: 0.0093\n",
      "internel Epoch [1/10], Loss: 0.0093\n",
      "internel Epoch [1/10], Loss: 0.0093\n",
      "internel Epoch [1/10], Loss: 0.0094\n",
      "internel Epoch [1/10], Loss: 0.0095\n",
      "internel Epoch [1/10], Loss: 0.0095\n",
      "internel Epoch [1/10], Loss: 0.0096\n",
      "internel Epoch [1/10], Loss: 0.0097\n",
      "internel Epoch [1/10], Loss: 0.0100\n",
      "internel Epoch [1/10], Loss: 0.0100\n",
      "internel Epoch [1/10], Loss: 0.0101\n",
      "internel Epoch [1/10], Loss: 0.0102\n",
      "internel Epoch [1/10], Loss: 0.0102\n",
      "internel Epoch [1/10], Loss: 0.0103\n",
      "internel Epoch [1/10], Loss: 0.0104\n",
      "internel Epoch [1/10], Loss: 0.0104\n",
      "internel Epoch [1/10], Loss: 0.0105\n",
      "internel Epoch [1/10], Loss: 0.0106\n",
      "internel Epoch [1/10], Loss: 0.0107\n",
      "internel Epoch [1/10], Loss: 0.0107\n",
      "internel Epoch [1/10], Loss: 0.0108\n",
      "internel Epoch [1/10], Loss: 0.0109\n",
      "internel Epoch [1/10], Loss: 0.0109\n",
      "internel Epoch [1/10], Loss: 0.0110\n",
      "internel Epoch [1/10], Loss: 0.0111\n",
      "internel Epoch [1/10], Loss: 0.0112\n",
      "internel Epoch [1/10], Loss: 0.0112\n",
      "internel Epoch [1/10], Loss: 0.0114\n",
      "internel Epoch [1/10], Loss: 0.0114\n",
      "internel Epoch [1/10], Loss: 0.0116\n",
      "internel Epoch [1/10], Loss: 0.0118\n",
      "internel Epoch [1/10], Loss: 0.0118\n",
      "internel Epoch [1/10], Loss: 0.0118\n",
      "internel Epoch [1/10], Loss: 0.0119\n",
      "internel Epoch [1/10], Loss: 0.0120\n",
      "internel Epoch [1/10], Loss: 0.0120\n",
      "internel Epoch [1/10], Loss: 0.0120\n",
      "internel Epoch [1/10], Loss: 0.0121\n",
      "internel Epoch [1/10], Loss: 0.0122\n",
      "internel Epoch [1/10], Loss: 0.0123\n",
      "internel Epoch [1/10], Loss: 0.0123\n",
      "internel Epoch [1/10], Loss: 0.0124\n",
      "internel Epoch [1/10], Loss: 0.0125\n",
      "internel Epoch [1/10], Loss: 0.0126\n",
      "internel Epoch [1/10], Loss: 0.0127\n",
      "internel Epoch [1/10], Loss: 0.0127\n",
      "internel Epoch [1/10], Loss: 0.0128\n",
      "internel Epoch [1/10], Loss: 0.0129\n",
      "internel Epoch [1/10], Loss: 0.0129\n",
      "internel Epoch [1/10], Loss: 0.0130\n",
      "internel Epoch [1/10], Loss: 0.0132\n",
      "internel Epoch [1/10], Loss: 0.0132\n",
      "internel Epoch [1/10], Loss: 0.0134\n",
      "internel Epoch [1/10], Loss: 0.0136\n",
      "internel Epoch [1/10], Loss: 0.0137\n",
      "internel Epoch [1/10], Loss: 0.0138\n",
      "internel Epoch [1/10], Loss: 0.0140\n",
      "internel Epoch [1/10], Loss: 0.0141\n",
      "internel Epoch [1/10], Loss: 0.0142\n",
      "internel Epoch [1/10], Loss: 0.0144\n",
      "internel Epoch [1/10], Loss: 0.0145\n",
      "internel Epoch [1/10], Loss: 0.0147\n",
      "internel Epoch [1/10], Loss: 0.0148\n",
      "internel Epoch [1/10], Loss: 0.0148\n",
      "internel Epoch [1/10], Loss: 0.0149\n",
      "internel Epoch [1/10], Loss: 0.0150\n",
      "internel Epoch [1/10], Loss: 0.0152\n",
      "internel Epoch [1/10], Loss: 0.0153\n",
      "internel Epoch [1/10], Loss: 0.0154\n",
      "internel Epoch [1/10], Loss: 0.0154\n",
      "internel Epoch [1/10], Loss: 0.0155\n",
      "internel Epoch [1/10], Loss: 0.0155\n",
      "internel Epoch [1/10], Loss: 0.0155\n",
      "internel Epoch [1/10], Loss: 0.0156\n",
      "internel Epoch [1/10], Loss: 0.0156\n",
      "internel Epoch [1/10], Loss: 0.0157\n",
      "internel Epoch [1/10], Loss: 0.0157\n",
      "internel Epoch [1/10], Loss: 0.0159\n",
      "internel Epoch [1/10], Loss: 0.0160\n",
      "internel Epoch [1/10], Loss: 0.0161\n",
      "internel Epoch [1/10], Loss: 0.0161\n",
      "internel Epoch [1/10], Loss: 0.0161\n",
      "internel Epoch [1/10], Loss: 0.0161\n",
      "internel Epoch [1/10], Loss: 0.0162\n",
      "internel Epoch [1/10], Loss: 0.0162\n",
      "internel Epoch [1/10], Loss: 0.0164\n",
      "internel Epoch [1/10], Loss: 0.0165\n",
      "internel Epoch [1/10], Loss: 0.0166\n",
      "internel Epoch [1/10], Loss: 0.0167\n",
      "internel Epoch [1/10], Loss: 0.0167\n",
      "internel Epoch [1/10], Loss: 0.0168\n",
      "internel Epoch [1/10], Loss: 0.0168\n",
      "internel Epoch [1/10], Loss: 0.0168\n",
      "internel Epoch [1/10], Loss: 0.0169\n",
      "internel Epoch [1/10], Loss: 0.0169\n",
      "internel Epoch [1/10], Loss: 0.0170\n",
      "internel Epoch [1/10], Loss: 0.0171\n",
      "internel Epoch [1/10], Loss: 0.0171\n",
      "internel Epoch [1/10], Loss: 0.0172\n",
      "internel Epoch [1/10], Loss: 0.0173\n",
      "internel Epoch [1/10], Loss: 0.0174\n",
      "internel Epoch [1/10], Loss: 0.0174\n",
      "internel Epoch [1/10], Loss: 0.0175\n",
      "internel Epoch [1/10], Loss: 0.0175\n",
      "internel Epoch [1/10], Loss: 0.0177\n",
      "internel Epoch [1/10], Loss: 0.0177\n",
      "internel Epoch [1/10], Loss: 0.0177\n",
      "internel Epoch [1/10], Loss: 0.0178\n",
      "internel Epoch [1/10], Loss: 0.0178\n",
      "internel Epoch [1/10], Loss: 0.0178\n",
      "internel Epoch [1/10], Loss: 0.0178\n",
      "internel Epoch [1/10], Loss: 0.0179\n",
      "internel Epoch [1/10], Loss: 0.0179\n",
      "internel Epoch [1/10], Loss: 0.0179\n",
      "internel Epoch [1/10], Loss: 0.0180\n",
      "internel Epoch [1/10], Loss: 0.0180\n",
      "internel Epoch [1/10], Loss: 0.0180\n",
      "internel Epoch [1/10], Loss: 0.0181\n",
      "internel Epoch [1/10], Loss: 0.0181\n",
      "internel Epoch [1/10], Loss: 0.0182\n",
      "internel Epoch [1/10], Loss: 0.0183\n",
      "internel Epoch [1/10], Loss: 0.0184\n",
      "internel Epoch [1/10], Loss: 0.0185\n",
      "internel Epoch [1/10], Loss: 0.0186\n",
      "internel Epoch [1/10], Loss: 0.0186\n",
      "internel Epoch [1/10], Loss: 0.0186\n",
      "internel Epoch [1/10], Loss: 0.0188\n",
      "internel Epoch [1/10], Loss: 0.0188\n",
      "internel Epoch [1/10], Loss: 0.0189\n",
      "internel Epoch [1/10], Loss: 0.0189\n",
      "internel Epoch [1/10], Loss: 0.0189\n",
      "internel Epoch [1/10], Loss: 0.0189\n",
      "internel Epoch [1/10], Loss: 0.0190\n",
      "internel Epoch [1/10], Loss: 0.0191\n",
      "internel Epoch [1/10], Loss: 0.0191\n",
      "internel Epoch [1/10], Loss: 0.0191\n",
      "internel Epoch [1/10], Loss: 0.0191\n",
      "internel Epoch [1/10], Loss: 0.0192\n",
      "internel Epoch [1/10], Loss: 0.0192\n",
      "internel Epoch [1/10], Loss: 0.0193\n",
      "internel Epoch [1/10], Loss: 0.0193\n",
      "internel Epoch [1/10], Loss: 0.0193\n",
      "internel Epoch [1/10], Loss: 0.0194\n",
      "internel Epoch [1/10], Loss: 0.0196\n",
      "internel Epoch [1/10], Loss: 0.0196\n",
      "internel Epoch [1/10], Loss: 0.0196\n",
      "internel Epoch [1/10], Loss: 0.0197\n",
      "internel Epoch [1/10], Loss: 0.0198\n",
      "internel Epoch [1/10], Loss: 0.0198\n",
      "internel Epoch [1/10], Loss: 0.0199\n",
      "internel Epoch [1/10], Loss: 0.0199\n",
      "internel Epoch [1/10], Loss: 0.0200\n",
      "internel Epoch [1/10], Loss: 0.0201\n",
      "internel Epoch [1/10], Loss: 0.0202\n",
      "internel Epoch [1/10], Loss: 0.0202\n",
      "internel Epoch [1/10], Loss: 0.0203\n",
      "internel Epoch [1/10], Loss: 0.0204\n",
      "internel Epoch [1/10], Loss: 0.0204\n",
      "internel Epoch [1/10], Loss: 0.0204\n",
      "internel Epoch [1/10], Loss: 0.0204\n",
      "internel Epoch [1/10], Loss: 0.0205\n",
      "internel Epoch [1/10], Loss: 0.0205\n",
      "internel Epoch [1/10], Loss: 0.0206\n",
      "internel Epoch [1/10], Loss: 0.0207\n",
      "internel Epoch [1/10], Loss: 0.0207\n",
      "internel Epoch [1/10], Loss: 0.0208\n",
      "internel Epoch [1/10], Loss: 0.0211\n",
      "internel Epoch [1/10], Loss: 0.0211\n",
      "internel Epoch [1/10], Loss: 0.0213\n",
      "internel Epoch [1/10], Loss: 0.0214\n",
      "internel Epoch [1/10], Loss: 0.0214\n",
      "internel Epoch [1/10], Loss: 0.0214\n",
      "internel Epoch [1/10], Loss: 0.0215\n",
      "internel Epoch [1/10], Loss: 0.0215\n",
      "internel Epoch [1/10], Loss: 0.0215\n",
      "internel Epoch [1/10], Loss: 0.0215\n",
      "internel Epoch [1/10], Loss: 0.0215\n",
      "internel Epoch [1/10], Loss: 0.0215\n",
      "internel Epoch [1/10], Loss: 0.0216\n",
      "internel Epoch [1/10], Loss: 0.0216\n",
      "internel Epoch [1/10], Loss: 0.0216\n",
      "internel Epoch [1/10], Loss: 0.0216\n",
      "internel Epoch [1/10], Loss: 0.0217\n",
      "internel Epoch [1/10], Loss: 0.0217\n",
      "internel Epoch [1/10], Loss: 0.0218\n",
      "internel Epoch [1/10], Loss: 0.0218\n",
      "internel Epoch [1/10], Loss: 0.0219\n",
      "internel Epoch [1/10], Loss: 0.0219\n",
      "internel Epoch [1/10], Loss: 0.0219\n",
      "internel Epoch [1/10], Loss: 0.0220\n",
      "internel Epoch [1/10], Loss: 0.0220\n",
      "internel Epoch [1/10], Loss: 0.0220\n",
      "internel Epoch [1/10], Loss: 0.0220\n",
      "internel Epoch [1/10], Loss: 0.0222\n",
      "internel Epoch [1/10], Loss: 0.0222\n",
      "internel Epoch [1/10], Loss: 0.0223\n",
      "internel Epoch [1/10], Loss: 0.0224\n",
      "internel Epoch [1/10], Loss: 0.0224\n",
      "internel Epoch [1/10], Loss: 0.0224\n",
      "internel Epoch [1/10], Loss: 0.0226\n",
      "internel Epoch [1/10], Loss: 0.0226\n",
      "internel Epoch [1/10], Loss: 0.0226\n",
      "internel Epoch [1/10], Loss: 0.0227\n",
      "internel Epoch [1/10], Loss: 0.0227\n",
      "internel Epoch [1/10], Loss: 0.0228\n",
      "internel Epoch [1/10], Loss: 0.0229\n",
      "internel Epoch [1/10], Loss: 0.0229\n",
      "internel Epoch [1/10], Loss: 0.0230\n",
      "internel Epoch [1/10], Loss: 0.0230\n",
      "internel Epoch [1/10], Loss: 0.0230\n",
      "internel Epoch [1/10], Loss: 0.0230\n",
      "internel Epoch [1/10], Loss: 0.0231\n",
      "internel Epoch [1/10], Loss: 0.0231\n",
      "internel Epoch [1/10], Loss: 0.0231\n",
      "internel Epoch [1/10], Loss: 0.0232\n",
      "internel Epoch [1/10], Loss: 0.0232\n",
      "internel Epoch [1/10], Loss: 0.0232\n",
      "internel Epoch [1/10], Loss: 0.0232\n",
      "internel Epoch [1/10], Loss: 0.0232\n",
      "internel Epoch [1/10], Loss: 0.0232\n",
      "internel Epoch [1/10], Loss: 0.0233\n",
      "internel Epoch [1/10], Loss: 0.0233\n",
      "internel Epoch [1/10], Loss: 0.0234\n",
      "internel Epoch [1/10], Loss: 0.0235\n",
      "internel Epoch [1/10], Loss: 0.0235\n",
      "internel Epoch [1/10], Loss: 0.0235\n",
      "internel Epoch [1/10], Loss: 0.0236\n",
      "internel Epoch [1/10], Loss: 0.0236\n",
      "internel Epoch [1/10], Loss: 0.0236\n",
      "internel Epoch [1/10], Loss: 0.0238\n",
      "internel Epoch [1/10], Loss: 0.0238\n",
      "internel Epoch [1/10], Loss: 0.0238\n",
      "internel Epoch [1/10], Loss: 0.0238\n",
      "internel Epoch [1/10], Loss: 0.0239\n",
      "internel Epoch [1/10], Loss: 0.0239\n",
      "internel Epoch [1/10], Loss: 0.0240\n",
      "internel Epoch [1/10], Loss: 0.0240\n",
      "internel Epoch [1/10], Loss: 0.0241\n",
      "internel Epoch [1/10], Loss: 0.0241\n",
      "internel Epoch [1/10], Loss: 0.0241\n",
      "internel Epoch [1/10], Loss: 0.0241\n",
      "internel Epoch [1/10], Loss: 0.0241\n",
      "internel Epoch [1/10], Loss: 0.0241\n",
      "internel Epoch [1/10], Loss: 0.0242\n",
      "internel Epoch [1/10], Loss: 0.0243\n",
      "internel Epoch [1/10], Loss: 0.0243\n",
      "internel Epoch [1/10], Loss: 0.0244\n",
      "internel Epoch [1/10], Loss: 0.0246\n",
      "internel Epoch [1/10], Loss: 0.0248\n",
      "internel Epoch [1/10], Loss: 0.0248\n",
      "internel Epoch [1/10], Loss: 0.0249\n",
      "internel Epoch [1/10], Loss: 0.0250\n",
      "internel Epoch [1/10], Loss: 0.0250\n",
      "internel Epoch [1/10], Loss: 0.0251\n",
      "internel Epoch [1/10], Loss: 0.0251\n",
      "internel Epoch [1/10], Loss: 0.0251\n",
      "internel Epoch [1/10], Loss: 0.0252\n",
      "internel Epoch [1/10], Loss: 0.0253\n",
      "internel Epoch [1/10], Loss: 0.0253\n",
      "internel Epoch [1/10], Loss: 0.0254\n",
      "internel Epoch [1/10], Loss: 0.0254\n",
      "internel Epoch [1/10], Loss: 0.0254\n",
      "internel Epoch [1/10], Loss: 0.0254\n",
      "internel Epoch [1/10], Loss: 0.0255\n",
      "internel Epoch [1/10], Loss: 0.0255\n",
      "internel Epoch [1/10], Loss: 0.0256\n",
      "internel Epoch [1/10], Loss: 0.0257\n",
      "internel Epoch [1/10], Loss: 0.0257\n",
      "internel Epoch [1/10], Loss: 0.0259\n",
      "internel Epoch [1/10], Loss: 0.0259\n",
      "internel Epoch [1/10], Loss: 0.0260\n",
      "internel Epoch [1/10], Loss: 0.0260\n",
      "internel Epoch [1/10], Loss: 0.0260\n",
      "internel Epoch [1/10], Loss: 0.0262\n",
      "internel Epoch [1/10], Loss: 0.0262\n",
      "internel Epoch [1/10], Loss: 0.0263\n",
      "internel Epoch [1/10], Loss: 0.0265\n",
      "internel Epoch [1/10], Loss: 0.0265\n",
      "internel Epoch [1/10], Loss: 0.0266\n",
      "internel Epoch [1/10], Loss: 0.0266\n",
      "internel Epoch [1/10], Loss: 0.0266\n",
      "internel Epoch [1/10], Loss: 0.0267\n",
      "internel Epoch [1/10], Loss: 0.0267\n",
      "internel Epoch [1/10], Loss: 0.0267\n",
      "internel Epoch [1/10], Loss: 0.0269\n",
      "internel Epoch [1/10], Loss: 0.0269\n",
      "internel Epoch [1/10], Loss: 0.0269\n",
      "internel Epoch [1/10], Loss: 0.0270\n",
      "internel Epoch [1/10], Loss: 0.0271\n",
      "internel Epoch [1/10], Loss: 0.0271\n",
      "internel Epoch [1/10], Loss: 0.0272\n",
      "internel Epoch [1/10], Loss: 0.0272\n",
      "internel Epoch [1/10], Loss: 0.0273\n",
      "internel Epoch [1/10], Loss: 0.0273\n",
      "internel Epoch [1/10], Loss: 0.0274\n",
      "internel Epoch [1/10], Loss: 0.0275\n",
      "internel Epoch [1/10], Loss: 0.0275\n",
      "internel Epoch [1/10], Loss: 0.0275\n",
      "internel Epoch [1/10], Loss: 0.0275\n",
      "internel Epoch [1/10], Loss: 0.0275\n",
      "internel Epoch [1/10], Loss: 0.0275\n",
      "internel Epoch [1/10], Loss: 0.0276\n",
      "internel Epoch [1/10], Loss: 0.0277\n",
      "internel Epoch [1/10], Loss: 0.0277\n",
      "internel Epoch [1/10], Loss: 0.0277\n",
      "internel Epoch [1/10], Loss: 0.0277\n",
      "internel Epoch [1/10], Loss: 0.0278\n",
      "internel Epoch [1/10], Loss: 0.0278\n",
      "internel Epoch [1/10], Loss: 0.0279\n",
      "internel Epoch [1/10], Loss: 0.0280\n",
      "internel Epoch [1/10], Loss: 0.0280\n",
      "internel Epoch [1/10], Loss: 0.0280\n",
      "internel Epoch [1/10], Loss: 0.0281\n",
      "internel Epoch [1/10], Loss: 0.0281\n",
      "internel Epoch [1/10], Loss: 0.0281\n",
      "internel Epoch [1/10], Loss: 0.0281\n",
      "internel Epoch [1/10], Loss: 0.0282\n",
      "internel Epoch [1/10], Loss: 0.0283\n",
      "internel Epoch [1/10], Loss: 0.0283\n",
      "internel Epoch [1/10], Loss: 0.0283\n",
      "internel Epoch [1/10], Loss: 0.0285\n",
      "internel Epoch [1/10], Loss: 0.0285\n",
      "internel Epoch [1/10], Loss: 0.0285\n",
      "internel Epoch [1/10], Loss: 0.0286\n",
      "internel Epoch [1/10], Loss: 0.0286\n",
      "internel Epoch [1/10], Loss: 0.0286\n",
      "internel Epoch [1/10], Loss: 0.0287\n",
      "internel Epoch [1/10], Loss: 0.0287\n",
      "internel Epoch [1/10], Loss: 0.0288\n",
      "internel Epoch [1/10], Loss: 0.0288\n",
      "internel Epoch [1/10], Loss: 0.0288\n",
      "internel Epoch [1/10], Loss: 0.0289\n",
      "internel Epoch [1/10], Loss: 0.0289\n",
      "internel Epoch [1/10], Loss: 0.0289\n",
      "internel Epoch [1/10], Loss: 0.0290\n",
      "internel Epoch [1/10], Loss: 0.0290\n",
      "internel Epoch [1/10], Loss: 0.0290\n",
      "internel Epoch [1/10], Loss: 0.0290\n",
      "internel Epoch [1/10], Loss: 0.0292\n",
      "internel Epoch [1/10], Loss: 0.0292\n",
      "internel Epoch [1/10], Loss: 0.0294\n",
      "internel Epoch [1/10], Loss: 0.0295\n",
      "internel Epoch [1/10], Loss: 0.0295\n",
      "internel Epoch [1/10], Loss: 0.0295\n",
      "internel Epoch [1/10], Loss: 0.0296\n",
      "internel Epoch [1/10], Loss: 0.0297\n",
      "internel Epoch [1/10], Loss: 0.0297\n",
      "internel Epoch [1/10], Loss: 0.0297\n",
      "internel Epoch [1/10], Loss: 0.0297\n",
      "internel Epoch [1/10], Loss: 0.0298\n",
      "internel Epoch [1/10], Loss: 0.0300\n",
      "internel Epoch [1/10], Loss: 0.0301\n",
      "internel Epoch [1/10], Loss: 0.0301\n",
      "internel Epoch [1/10], Loss: 0.0302\n",
      "internel Epoch [1/10], Loss: 0.0303\n",
      "internel Epoch [1/10], Loss: 0.0303\n",
      "internel Epoch [1/10], Loss: 0.0304\n",
      "internel Epoch [1/10], Loss: 0.0304\n",
      "internel Epoch [1/10], Loss: 0.0305\n",
      "internel Epoch [1/10], Loss: 0.0306\n",
      "internel Epoch [1/10], Loss: 0.0306\n",
      "internel Epoch [1/10], Loss: 0.0307\n",
      "internel Epoch [1/10], Loss: 0.0308\n",
      "internel Epoch [1/10], Loss: 0.0309\n",
      "internel Epoch [1/10], Loss: 0.0309\n",
      "internel Epoch [1/10], Loss: 0.0309\n",
      "internel Epoch [1/10], Loss: 0.0309\n",
      "internel Epoch [1/10], Loss: 0.0309\n",
      "internel Epoch [1/10], Loss: 0.0310\n",
      "internel Epoch [1/10], Loss: 0.0310\n",
      "internel Epoch [1/10], Loss: 0.0310\n",
      "internel Epoch [1/10], Loss: 0.0311\n",
      "internel Epoch [1/10], Loss: 0.0311\n",
      "internel Epoch [1/10], Loss: 0.0311\n",
      "internel Epoch [1/10], Loss: 0.0313\n",
      "internel Epoch [1/10], Loss: 0.0313\n",
      "internel Epoch [1/10], Loss: 0.0313\n",
      "internel Epoch [1/10], Loss: 0.0314\n",
      "internel Epoch [1/10], Loss: 0.0315\n",
      "internel Epoch [1/10], Loss: 0.0315\n",
      "internel Epoch [1/10], Loss: 0.0316\n",
      "internel Epoch [1/10], Loss: 0.0317\n",
      "internel Epoch [1/10], Loss: 0.0317\n",
      "internel Epoch [1/10], Loss: 0.0317\n",
      "internel Epoch [1/10], Loss: 0.0318\n",
      "internel Epoch [1/10], Loss: 0.0319\n",
      "internel Epoch [1/10], Loss: 0.0321\n",
      "internel Epoch [1/10], Loss: 0.0322\n",
      "internel Epoch [1/10], Loss: 0.0322\n",
      "internel Epoch [1/10], Loss: 0.0322\n",
      "internel Epoch [1/10], Loss: 0.0322\n",
      "internel Epoch [1/10], Loss: 0.0323\n",
      "internel Epoch [1/10], Loss: 0.0323\n",
      "internel Epoch [1/10], Loss: 0.0324\n",
      "internel Epoch [1/10], Loss: 0.0325\n",
      "internel Epoch [1/10], Loss: 0.0326\n",
      "internel Epoch [1/10], Loss: 0.0326\n",
      "internel Epoch [1/10], Loss: 0.0326\n",
      "internel Epoch [1/10], Loss: 0.0327\n",
      "internel Epoch [1/10], Loss: 0.0328\n",
      "internel Epoch [1/10], Loss: 0.0328\n",
      "internel Epoch [1/10], Loss: 0.0328\n",
      "internel Epoch [1/10], Loss: 0.0328\n",
      "internel Epoch [1/10], Loss: 0.0328\n",
      "internel Epoch [1/10], Loss: 0.0329\n",
      "internel Epoch [1/10], Loss: 0.0329\n",
      "internel Epoch [1/10], Loss: 0.0330\n",
      "internel Epoch [1/10], Loss: 0.0332\n",
      "internel Epoch [1/10], Loss: 0.0333\n",
      "internel Epoch [1/10], Loss: 0.0333\n",
      "internel Epoch [1/10], Loss: 0.0333\n",
      "internel Epoch [1/10], Loss: 0.0335\n",
      "internel Epoch [1/10], Loss: 0.0336\n",
      "internel Epoch [1/10], Loss: 0.0336\n",
      "internel Epoch [1/10], Loss: 0.0336\n",
      "internel Epoch [1/10], Loss: 0.0336\n",
      "internel Epoch [1/10], Loss: 0.0336\n",
      "internel Epoch [1/10], Loss: 0.0337\n",
      "internel Epoch [1/10], Loss: 0.0337\n",
      "internel Epoch [1/10], Loss: 0.0337\n",
      "internel Epoch [1/10], Loss: 0.0337\n",
      "internel Epoch [1/10], Loss: 0.0338\n",
      "internel Epoch [1/10], Loss: 0.0338\n",
      "internel Epoch [1/10], Loss: 0.0339\n",
      "internel Epoch [1/10], Loss: 0.0339\n",
      "internel Epoch [1/10], Loss: 0.0340\n",
      "internel Epoch [1/10], Loss: 0.0341\n",
      "internel Epoch [1/10], Loss: 0.0342\n",
      "internel Epoch [1/10], Loss: 0.0342\n",
      "internel Epoch [1/10], Loss: 0.0343\n",
      "internel Epoch [1/10], Loss: 0.0344\n",
      "internel Epoch [1/10], Loss: 0.0344\n",
      "internel Epoch [1/10], Loss: 0.0344\n",
      "internel Epoch [1/10], Loss: 0.0345\n",
      "internel Epoch [1/10], Loss: 0.0346\n",
      "internel Epoch [1/10], Loss: 0.0347\n",
      "internel Epoch [1/10], Loss: 0.0347\n",
      "internel Epoch [1/10], Loss: 0.0347\n",
      "internel Epoch [1/10], Loss: 0.0347\n",
      "internel Epoch [1/10], Loss: 0.0349\n",
      "internel Epoch [1/10], Loss: 0.0349\n",
      "internel Epoch [1/10], Loss: 0.0350\n",
      "internel Epoch [1/10], Loss: 0.0350\n",
      "internel Epoch [1/10], Loss: 0.0350\n",
      "internel Epoch [1/10], Loss: 0.0350\n",
      "internel Epoch [1/10], Loss: 0.0351\n",
      "internel Epoch [1/10], Loss: 0.0351\n",
      "internel Epoch [1/10], Loss: 0.0352\n",
      "internel Epoch [1/10], Loss: 0.0352\n",
      "internel Epoch [1/10], Loss: 0.0352\n",
      "internel Epoch [1/10], Loss: 0.0352\n",
      "internel Epoch [1/10], Loss: 0.0353\n",
      "internel Epoch [1/10], Loss: 0.0353\n",
      "internel Epoch [1/10], Loss: 0.0353\n",
      "internel Epoch [1/10], Loss: 0.0353\n",
      "internel Epoch [1/10], Loss: 0.0353\n",
      "internel Epoch [1/10], Loss: 0.0353\n",
      "internel Epoch [1/10], Loss: 0.0353\n",
      "internel Epoch [1/10], Loss: 0.0354\n",
      "internel Epoch [1/10], Loss: 0.0354\n",
      "internel Epoch [1/10], Loss: 0.0354\n",
      "internel Epoch [1/10], Loss: 0.0355\n",
      "internel Epoch [1/10], Loss: 0.0355\n",
      "internel Epoch [1/10], Loss: 0.0355\n",
      "internel Epoch [1/10], Loss: 0.0355\n",
      "internel Epoch [1/10], Loss: 0.0355\n",
      "internel Epoch [1/10], Loss: 0.0355\n",
      "internel Epoch [1/10], Loss: 0.0356\n",
      "internel Epoch [1/10], Loss: 0.0356\n",
      "internel Epoch [1/10], Loss: 0.0356\n",
      "internel Epoch [1/10], Loss: 0.0356\n",
      "internel Epoch [1/10], Loss: 0.0357\n",
      "internel Epoch [1/10], Loss: 0.0357\n",
      "internel Epoch [1/10], Loss: 0.0358\n",
      "internel Epoch [1/10], Loss: 0.0358\n",
      "internel Epoch [1/10], Loss: 0.0358\n",
      "internel Epoch [1/10], Loss: 0.0359\n",
      "internel Epoch [1/10], Loss: 0.0360\n",
      "internel Epoch [1/10], Loss: 0.0360\n",
      "internel Epoch [1/10], Loss: 0.0361\n",
      "internel Epoch [1/10], Loss: 0.0361\n",
      "internel Epoch [1/10], Loss: 0.0361\n",
      "internel Epoch [1/10], Loss: 0.0361\n",
      "internel Epoch [1/10], Loss: 0.0361\n",
      "internel Epoch [1/10], Loss: 0.0361\n",
      "internel Epoch [1/10], Loss: 0.0362\n",
      "internel Epoch [1/10], Loss: 0.0362\n",
      "internel Epoch [1/10], Loss: 0.0362\n",
      "internel Epoch [1/10], Loss: 0.0363\n",
      "internel Epoch [1/10], Loss: 0.0363\n",
      "internel Epoch [1/10], Loss: 0.0364\n",
      "internel Epoch [1/10], Loss: 0.0364\n",
      "internel Epoch [1/10], Loss: 0.0364\n",
      "internel Epoch [1/10], Loss: 0.0365\n",
      "internel Epoch [1/10], Loss: 0.0365\n",
      "internel Epoch [1/10], Loss: 0.0365\n",
      "internel Epoch [1/10], Loss: 0.0365\n",
      "internel Epoch [1/10], Loss: 0.0365\n",
      "internel Epoch [1/10], Loss: 0.0365\n",
      "internel Epoch [1/10], Loss: 0.0366\n",
      "internel Epoch [1/10], Loss: 0.0366\n",
      "internel Epoch [1/10], Loss: 0.0367\n",
      "internel Epoch [1/10], Loss: 0.0367\n",
      "internel Epoch [1/10], Loss: 0.0367\n",
      "internel Epoch [1/10], Loss: 0.0367\n",
      "internel Epoch [1/10], Loss: 0.0367\n",
      "internel Epoch [1/10], Loss: 0.0367\n",
      "internel Epoch [1/10], Loss: 0.0368\n",
      "internel Epoch [1/10], Loss: 0.0368\n",
      "internel Epoch [1/10], Loss: 0.0368\n",
      "internel Epoch [1/10], Loss: 0.0368\n",
      "internel Epoch [1/10], Loss: 0.0369\n",
      "internel Epoch [1/10], Loss: 0.0370\n",
      "internel Epoch [1/10], Loss: 0.0371\n",
      "internel Epoch [1/10], Loss: 0.0372\n",
      "internel Epoch [1/10], Loss: 0.0372\n",
      "internel Epoch [1/10], Loss: 0.0373\n",
      "internel Epoch [1/10], Loss: 0.0373\n",
      "internel Epoch [1/10], Loss: 0.0374\n",
      "internel Epoch [1/10], Loss: 0.0374\n",
      "internel Epoch [1/10], Loss: 0.0375\n",
      "internel Epoch [1/10], Loss: 0.0375\n",
      "internel Epoch [1/10], Loss: 0.0375\n",
      "internel Epoch [1/10], Loss: 0.0376\n",
      "internel Epoch [1/10], Loss: 0.0376\n",
      "internel Epoch [1/10], Loss: 0.0377\n",
      "internel Epoch [1/10], Loss: 0.0377\n",
      "internel Epoch [1/10], Loss: 0.0378\n",
      "internel Epoch [1/10], Loss: 0.0378\n",
      "internel Epoch [1/10], Loss: 0.0378\n",
      "internel Epoch [1/10], Loss: 0.0379\n",
      "internel Epoch [1/10], Loss: 0.0379\n",
      "internel Epoch [1/10], Loss: 0.0381\n",
      "internel Epoch [1/10], Loss: 0.0381\n",
      "internel Epoch [1/10], Loss: 0.0381\n",
      "internel Epoch [1/10], Loss: 0.0381\n",
      "internel Epoch [1/10], Loss: 0.0381\n",
      "internel Epoch [1/10], Loss: 0.0382\n",
      "internel Epoch [1/10], Loss: 0.0382\n",
      "internel Epoch [1/10], Loss: 0.0382\n",
      "internel Epoch [1/10], Loss: 0.0383\n",
      "internel Epoch [1/10], Loss: 0.0384\n",
      "internel Epoch [1/10], Loss: 0.0384\n",
      "internel Epoch [1/10], Loss: 0.0385\n",
      "internel Epoch [1/10], Loss: 0.0387\n",
      "internel Epoch [1/10], Loss: 0.0387\n",
      "internel Epoch [1/10], Loss: 0.0388\n",
      "internel Epoch [1/10], Loss: 0.0388\n",
      "internel Epoch [1/10], Loss: 0.0388\n",
      "internel Epoch [1/10], Loss: 0.0389\n",
      "internel Epoch [1/10], Loss: 0.0390\n",
      "internel Epoch [1/10], Loss: 0.0390\n",
      "internel Epoch [1/10], Loss: 0.0391\n",
      "internel Epoch [1/10], Loss: 0.0392\n",
      "internel Epoch [1/10], Loss: 0.0392\n",
      "internel Epoch [1/10], Loss: 0.0392\n",
      "internel Epoch [1/10], Loss: 0.0392\n",
      "internel Epoch [1/10], Loss: 0.0393\n",
      "internel Epoch [1/10], Loss: 0.0394\n",
      "internel Epoch [1/10], Loss: 0.0394\n",
      "internel Epoch [1/10], Loss: 0.0394\n",
      "internel Epoch [1/10], Loss: 0.0394\n",
      "internel Epoch [1/10], Loss: 0.0395\n",
      "internel Epoch [1/10], Loss: 0.0395\n",
      "internel Epoch [1/10], Loss: 0.0395\n",
      "internel Epoch [1/10], Loss: 0.0396\n",
      "internel Epoch [1/10], Loss: 0.0397\n",
      "internel Epoch [1/10], Loss: 0.0398\n",
      "internel Epoch [1/10], Loss: 0.0398\n",
      "internel Epoch [1/10], Loss: 0.0398\n",
      "internel Epoch [1/10], Loss: 0.0398\n",
      "internel Epoch [1/10], Loss: 0.0398\n",
      "internel Epoch [1/10], Loss: 0.0399\n",
      "internel Epoch [1/10], Loss: 0.0399\n",
      "internel Epoch [1/10], Loss: 0.0399\n",
      "internel Epoch [1/10], Loss: 0.0399\n",
      "internel Epoch [1/10], Loss: 0.0399\n",
      "internel Epoch [1/10], Loss: 0.0400\n",
      "internel Epoch [1/10], Loss: 0.0401\n",
      "internel Epoch [1/10], Loss: 0.0402\n",
      "internel Epoch [1/10], Loss: 0.0402\n",
      "internel Epoch [1/10], Loss: 0.0403\n",
      "internel Epoch [1/10], Loss: 0.0403\n",
      "internel Epoch [1/10], Loss: 0.0403\n",
      "internel Epoch [1/10], Loss: 0.0403\n",
      "internel Epoch [1/10], Loss: 0.0403\n",
      "internel Epoch [1/10], Loss: 0.0404\n",
      "internel Epoch [1/10], Loss: 0.0405\n",
      "internel Epoch [1/10], Loss: 0.0406\n",
      "internel Epoch [1/10], Loss: 0.0406\n",
      "internel Epoch [1/10], Loss: 0.0406\n",
      "internel Epoch [1/10], Loss: 0.0407\n",
      "internel Epoch [1/10], Loss: 0.0407\n",
      "internel Epoch [1/10], Loss: 0.0408\n",
      "internel Epoch [1/10], Loss: 0.0408\n",
      "internel Epoch [1/10], Loss: 0.0408\n",
      "internel Epoch [1/10], Loss: 0.0409\n",
      "internel Epoch [1/10], Loss: 0.0409\n",
      "internel Epoch [1/10], Loss: 0.0409\n",
      "internel Epoch [1/10], Loss: 0.0409\n",
      "internel Epoch [1/10], Loss: 0.0410\n",
      "internel Epoch [1/10], Loss: 0.0410\n",
      "internel Epoch [1/10], Loss: 0.0410\n",
      "internel Epoch [1/10], Loss: 0.0410\n",
      "internel Epoch [1/10], Loss: 0.0410\n",
      "internel Epoch [1/10], Loss: 0.0411\n",
      "internel Epoch [1/10], Loss: 0.0411\n",
      "internel Epoch [1/10], Loss: 0.0411\n",
      "internel Epoch [1/10], Loss: 0.0411\n",
      "internel Epoch [1/10], Loss: 0.0411\n",
      "internel Epoch [1/10], Loss: 0.0412\n",
      "internel Epoch [1/10], Loss: 0.0412\n",
      "internel Epoch [1/10], Loss: 0.0412\n",
      "internel Epoch [1/10], Loss: 0.0412\n",
      "internel Epoch [1/10], Loss: 0.0412\n",
      "internel Epoch [1/10], Loss: 0.0413\n",
      "internel Epoch [1/10], Loss: 0.0414\n",
      "internel Epoch [1/10], Loss: 0.0415\n",
      "internel Epoch [1/10], Loss: 0.0415\n",
      "internel Epoch [1/10], Loss: 0.0415\n",
      "internel Epoch [1/10], Loss: 0.0416\n",
      "internel Epoch [1/10], Loss: 0.0416\n",
      "internel Epoch [1/10], Loss: 0.0417\n",
      "internel Epoch [1/10], Loss: 0.0417\n",
      "internel Epoch [1/10], Loss: 0.0417\n",
      "internel Epoch [1/10], Loss: 0.0418\n",
      "internel Epoch [1/10], Loss: 0.0418\n",
      "internel Epoch [1/10], Loss: 0.0419\n",
      "internel Epoch [1/10], Loss: 0.0419\n",
      "internel Epoch [1/10], Loss: 0.0421\n",
      "internel Epoch [1/10], Loss: 0.0423\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[7], line 29\u001b[0m\n\u001b[0;32m     26\u001b[0m             correct \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m (predicted \u001b[38;5;241m==\u001b[39m labels)\u001b[38;5;241m.\u001b[39msum()\u001b[38;5;241m.\u001b[39mitem()\n\u001b[0;32m     27\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mAccuracy of the model on the test images: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;241m100\u001b[39m\u001b[38;5;250m \u001b[39m\u001b[38;5;241m*\u001b[39m\u001b[38;5;250m \u001b[39mcorrect\u001b[38;5;250m \u001b[39m\u001b[38;5;241m/\u001b[39m\u001b[38;5;250m \u001b[39mtotal\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m %\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m---> 29\u001b[0m \u001b[43mtrain_model\u001b[49m\u001b[43m(\u001b[49m\u001b[43mresnet_model\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mresnet_optimizer\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     30\u001b[0m evaluate_model(resnet_model)\n\u001b[0;32m     32\u001b[0m train_model(alexnet_model, alexnet_optimizer)\n",
      "Cell \u001b[1;32mIn[7], line 8\u001b[0m, in \u001b[0;36mtrain_model\u001b[1;34m(model, optimizer, num_epochs)\u001b[0m\n\u001b[0;32m      6\u001b[0m images, labels \u001b[38;5;241m=\u001b[39m images\u001b[38;5;241m.\u001b[39mto(device), labels\u001b[38;5;241m.\u001b[39mto(device)\n\u001b[0;32m      7\u001b[0m optimizer\u001b[38;5;241m.\u001b[39mzero_grad()\n\u001b[1;32m----> 8\u001b[0m outputs \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[43mimages\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m      9\u001b[0m loss \u001b[38;5;241m=\u001b[39m criterion(outputs, labels)\n\u001b[0;32m     10\u001b[0m loss\u001b[38;5;241m.\u001b[39mbackward()\n",
      "File \u001b[1;32mc:\\Users\\pasin\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1532\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1530\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1531\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1532\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\pasin\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1541\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1536\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1537\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1538\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1539\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1540\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1541\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1543\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m   1544\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "Cell \u001b[1;32mIn[4], line 10\u001b[0m, in \u001b[0;36mMNISTResNet.forward\u001b[1;34m(self, x)\u001b[0m\n\u001b[0;32m      9\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, x):\n\u001b[1;32m---> 10\u001b[0m     x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mresnet\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     11\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m x\n",
      "File \u001b[1;32mc:\\Users\\pasin\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1532\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1530\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1531\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1532\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\pasin\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1541\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1536\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1537\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1538\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1539\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1540\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1541\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1543\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m   1544\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\pasin\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\torchvision\\models\\resnet.py:285\u001b[0m, in \u001b[0;36mResNet.forward\u001b[1;34m(self, x)\u001b[0m\n\u001b[0;32m    284\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, x: Tensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tensor:\n\u001b[1;32m--> 285\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_forward_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\pasin\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\torchvision\\models\\resnet.py:273\u001b[0m, in \u001b[0;36mResNet._forward_impl\u001b[1;34m(self, x)\u001b[0m\n\u001b[0;32m    270\u001b[0m x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mrelu(x)\n\u001b[0;32m    271\u001b[0m x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmaxpool(x)\n\u001b[1;32m--> 273\u001b[0m x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlayer1\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    274\u001b[0m x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlayer2(x)\n\u001b[0;32m    275\u001b[0m x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlayer3(x)\n",
      "File \u001b[1;32mc:\\Users\\pasin\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1532\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1530\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1531\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1532\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\pasin\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1541\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1536\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1537\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1538\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1539\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1540\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1541\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1543\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m   1544\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\pasin\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\torch\\nn\\modules\\container.py:217\u001b[0m, in \u001b[0;36mSequential.forward\u001b[1;34m(self, input)\u001b[0m\n\u001b[0;32m    215\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m):\n\u001b[0;32m    216\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m module \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m:\n\u001b[1;32m--> 217\u001b[0m         \u001b[38;5;28minput\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[43mmodule\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m    218\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28minput\u001b[39m\n",
      "File \u001b[1;32mc:\\Users\\pasin\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1532\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1530\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1531\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1532\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\pasin\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1541\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1536\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1537\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1538\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1539\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1540\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1541\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1543\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m   1544\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\pasin\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\torchvision\\models\\resnet.py:92\u001b[0m, in \u001b[0;36mBasicBlock.forward\u001b[1;34m(self, x)\u001b[0m\n\u001b[0;32m     89\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, x: Tensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tensor:\n\u001b[0;32m     90\u001b[0m     identity \u001b[38;5;241m=\u001b[39m x\n\u001b[1;32m---> 92\u001b[0m     out \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconv1\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     93\u001b[0m     out \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbn1(out)\n\u001b[0;32m     94\u001b[0m     out \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mrelu(out)\n",
      "File \u001b[1;32mc:\\Users\\pasin\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1532\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1530\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1531\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1532\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\pasin\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1541\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1536\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1537\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1538\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1539\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1540\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1541\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1543\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m   1544\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\pasin\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\torch\\nn\\modules\\conv.py:460\u001b[0m, in \u001b[0;36mConv2d.forward\u001b[1;34m(self, input)\u001b[0m\n\u001b[0;32m    459\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m: Tensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tensor:\n\u001b[1;32m--> 460\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_conv_forward\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbias\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\pasin\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\torch\\nn\\modules\\conv.py:456\u001b[0m, in \u001b[0;36mConv2d._conv_forward\u001b[1;34m(self, input, weight, bias)\u001b[0m\n\u001b[0;32m    452\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpadding_mode \u001b[38;5;241m!=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mzeros\u001b[39m\u001b[38;5;124m'\u001b[39m:\n\u001b[0;32m    453\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m F\u001b[38;5;241m.\u001b[39mconv2d(F\u001b[38;5;241m.\u001b[39mpad(\u001b[38;5;28minput\u001b[39m, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_reversed_padding_repeated_twice, mode\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpadding_mode),\n\u001b[0;32m    454\u001b[0m                     weight, bias, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstride,\n\u001b[0;32m    455\u001b[0m                     _pair(\u001b[38;5;241m0\u001b[39m), \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdilation, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgroups)\n\u001b[1;32m--> 456\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconv2d\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbias\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstride\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    457\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpadding\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdilation\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgroups\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "def train_model(model, optimizer, num_epochs=10):\n",
    "    for epoch in range(num_epochs):\n",
    "        model.train()\n",
    "        running_loss = 0.0\n",
    "        for images, labels in train_loader:\n",
    "            images, labels = images.to(device), labels.to(device)\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(images)\n",
    "            loss = criterion(outputs, labels)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            running_loss += loss.item()\n",
    "            print(f\"internel Epoch [{epoch+1}/{num_epochs}], Loss: {running_loss/len(train_loader):.4f}\")\n",
    "        print(f\"Epoch [{epoch+1}/{num_epochs}], Loss: {running_loss/len(train_loader):.4f}\")\n",
    "\n",
    "def evaluate_model(model):\n",
    "    model.eval()\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    with torch.no_grad():\n",
    "        for images, labels in test_loader:\n",
    "            images, labels = images.to(device), labels.to(device)\n",
    "            outputs = model(images)\n",
    "            _, predicted = torch.max(outputs.data, 1)\n",
    "            total += labels.size(0)\n",
    "            correct += (predicted == labels).sum().item()\n",
    "    print(f'Accuracy of the model on the test images: {100 * correct / total} %')\n",
    "\n",
    "train_model(resnet_model, resnet_optimizer)\n",
    "evaluate_model(resnet_model)\n",
    "\n",
    "train_model(alexnet_model, alexnet_optimizer)\n",
    "evaluate_model(alexnet_model)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
